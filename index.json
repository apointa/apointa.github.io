[{"authors":null,"categories":null,"content":"About me\nHi, I am Andreas. I am a PhD student in the field of Machine Learning at the Institute for Machine Learning at Johannes Kepler University, Linz, Austria, advised by Sepp Hochreiter. I am also part of the ELLIS PhD Program.\nIn my research I am most interested in Deep Learning in the context of Time Series or, more broadly, sequential data. Currently I am specifically interested in Foundational Time Series Models:\nI led the development of TiRex, a state-of-the-art foundational forecasting model built with xLSTM, and COSMIC, the first foundational forecasting model that beneficially utilized covariates in a zero-shot setting. Further, I am the co-author of Chronos-2 and xLSTM.\nPrior to my PhD, I completed my BSc and MSc degrees in Computer Science at the Technical University of Vienna and gathered experience as professional software developer.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"About me\nHi, I am Andreas. I am a PhD student in the field of Machine Learning at the Institute for Machine Learning at Johannes Kepler University, Linz, Austria, advised by Sepp Hochreiter.","tags":null,"title":"Andreas Auer","type":"authors"},{"authors":["Abdul Fatir Ansari","Oleksandr Shchur","Jaris Küken","**Andreas Auer**","Boran Han","Pedro Mercado","Syama Sundar Rangapuram","Huibin Shen","Lorenzo Stella","Xiyuan Zhang","Mononito Goswami","Shubham Kapoor","Danielle C. Maddix","Pablo Guerron","Tony Hu","Junming Yin","Nick Erickson","Prateek Mutalik Desai","Hao Wang","Huzefa Rangwala","George Karypis","Yuyang Wang","Michael Bohlke-Schneider"],"categories":null,"content":"","date":1759276800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1759276800,"objectID":"c6dfeef09f2a8278e920f0c33872532b","permalink":"https://apointa.github.io/publication/2025-chronos2.html","publishdate":"2025-10-01T00:00:00Z","relpermalink":"/publication/2025-chronos2.html","section":"publication","summary":"Pretrained time series models have enabled inference-only forecasting systems that produce accurate predictions without task-specific training. However, existing approaches largely focus on univariate forecasting, limiting their applicability in real-world scenarios where multivariate data and covariates play a crucial role. We present Chronos-2, a pretrained model capable of handling univariate, multivariate, and covariate-informed forecasting tasks in a zero-shot manner. Chronos-2 employs a group attention mechanism that facilitates in-context learning (ICL) through efficient information sharing across multiple time series within a group, which may represent sets of related series, variates of a multivariate series, or targets and covariates in a forecasting task. These general capabilities are achieved through training on synthetic datasets that impose diverse multivariate structures on univariate series. Chronos-2 delivers state-of-the-art performance across three comprehensive benchmarks: fev-bench, GIFT-Eval, and Chronos Benchmark II. On fev-bench, which emphasizes multivariate and covariate-informed forecasting, Chronos-2's universal ICL capabilities lead to substantial improvements over existing models. On tasks involving covariates, it consistently outperforms baselines by a wide margin. Case studies in the energy and retail domains further highlight its practical advantages. The in-context learning capabilities of Chronos-2 establish it as a general-purpose forecasting model that can be used \"as is\" in real-world forecasting pipelines.","tags":null,"title":"Chronos-2: From Univariate to Universal Forecasting","type":"publication"},{"authors":["**Andreas Auer**, Daniel Klotz, Sebastian Böck, Sepp Hochreiter"],"categories":null,"content":"","date":1756684800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1756684800,"objectID":"1a270e88d0523d62dd39ef253026f525","permalink":"https://apointa.github.io/publication/2025-zs-classification.html","publishdate":"2025-09-01T00:00:00Z","relpermalink":"/publication/2025-zs-classification.html","section":"publication","summary":"Recent research on time series foundation models has primarily focused on forecasting, leaving it unclear how generalizable their learned representations are. In this study, we examine whether frozen pre-trained forecasting models can provide effective representations for classification. To this end, we compare different representation extraction strategies and introduce two model-agnostic embedding augmentations. Our experiments show that the best forecasting models achieve classification accuracy that matches or even surpasses that of state-of-the-art models pre-trained specifically for classification. Moreover, we observe a positive correlation between forecasting and classification performance. These findings challenge the assumption that task-specific pre-training is necessary, and suggest that learning to forecast may provide a powerful route toward constructing general-purpose time series foundation models.","tags":null,"title":"Pre-trained Forecasting Models: Strong Zero-Shot Feature Extractors for Time Series Classification","type":"publication"},{"authors":["**Andreas Auer**, Patrick Podest, Daniel Klotz, Sebastian Böck, Günter Klambauer, Sepp Hochreiter"],"categories":null,"content":"","date":1754006400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1754006400,"objectID":"1246809435c9e671144e7f7b81174e2f","permalink":"https://apointa.github.io/publication/2025-tirex.html","publishdate":"2025-08-01T00:00:00Z","relpermalink":"/publication/2025-tirex.html","section":"publication","summary":"In-context learning, the ability of large language models to perform tasks using only examples provided in the prompt, has recently been adapted for time series forecasting. This paradigm enables zero-shot prediction, where past values serve as context for forecasting future values, making powerful forecasting tools accessible to non-experts and increasing the performance when training data are scarce. Most existing zero-shot forecasting approaches rely on transformer architectures, which, despite their success in language, often fall short of expectations in time series forecasting, where recurrent models like LSTMs frequently have the edge. Conversely, while LSTMs are well-suited for time series modeling due to their state-tracking capabilities, they lack strong in-context learning abilities. We introduce TiRex that closes this gap by leveraging xLSTM, an enhanced LSTM with competitive in-context learning skills. Unlike transformers, state-space models, or parallelizable RNNs such as RWKV, TiRex retains state-tracking, a critical property for long-horizon forecasting. To further facilitate its state-tracking ability, we propose a training-time masking strategy called CPM. TiRex sets a new state of the art in zero-shot time series forecasting on the HuggingFace benchmarks GiftEval and Chronos-ZS, outperforming significantly larger models including TabPFN-TS (Prior Labs), Chronos Bolt (Amazon), TimesFM (Google), and Moirai (Salesforce) across both short- and long-term forecasts.","tags":null,"title":"TiRex: Zero-Shot Forecasting Across Long and Short Horizons with Enhanced In-Context Learning","type":"publication"},{"authors":["**Andreas Auer**, Raghul Parthipan, Pedro Mercado, Abdul Fatir Ansari, Lorenzo Stella, Bernie Wang, Michael Bohlke-Schneider, Syama Sundar Rangapuram"],"categories":null,"content":"","date":1748995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1748995200,"objectID":"fae1c2e02c6d6babab7b09c66b4d3260","permalink":"https://apointa.github.io/publication/2025-zero-cov.html","publishdate":"2025-06-04T00:00:00Z","relpermalink":"/publication/2025-zero-cov.html","section":"publication","summary":"Pretrained time series models, capable of zero-shot forecasting, have demonstrated significant potential in enhancing both the performance and accessibility of time series forecasting. However, existing pretrained models either do not support covariates or fail to incorporate them effectively. We introduce COSMIC, a zero-shot forecasting model that utilizes covariates via in-context learning. To address the challenge of data scarcity, we propose Informative Covariate Augmentation, which enables the training of COSMIC without requiring any datasets that include covariates. COSMIC achieves state-of-the-art performance in zero-shot forecasting, both with and without covariates. Our quantitative and qualitative analysis demonstrates that COSMIC effectively leverages covariates in zero-shot forecasting.","tags":null,"title":"Zero-Shot Time Series Forecasting with Covariates via In-Context Learning","type":"publication"},{"authors":["**Andreas Auer**, Patrick Podest, Daniel Klotz, Sebastian Böck, Günter Klambauer, Sepp Hochreiter"],"categories":null,"content":"","date":1748736e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1748736e3,"objectID":"98df4d6530a7048af36cb0eac27791a4","permalink":"https://apointa.github.io/publication/2025-tirex-workshop.html","publishdate":"2025-06-01T00:00:00Z","relpermalink":"/publication/2025-tirex-workshop.html","section":"publication","summary":"In-context learning, the ability of large language models to perform tasks using only examples provided in the prompt, has recently been adapted for time series forecasting. This paradigm enables zero-shot prediction, where past values serve as context for forecasting future values, making powerful forecasting tools accessible to non-experts and increasing the performance when training data are scarce. Most existing zero-shot forecasting approaches rely on transformer architectures, which, despite their success in language, often fall short of expectations in time series forecasting, where recurrent models like LSTMs frequently have the edge. Conversely, while LSTMs are well-suited for time series modeling due to their state-tracking capabilities, they lack strong in-context learning abilities. We introduce TiRex that closes this gap by leveraging xLSTM, an enhanced LSTM with competitive in-context learning skills. Unlike transformers, state-space models, or parallelizable RNNs such as RWKV, TiRex retains state-tracking, a critical property for long-horizon forecasting. To further facilitate its state-tracking ability, we propose a training-time masking strategy called CPM. TiRex sets a new state of the art in zero-shot time series forecasting on the HuggingFace benchmarks GiftEval and Chronos-ZS, outperforming significantly larger models including TabPFN-TS (Prior Labs), Chronos Bolt (Amazon), TimesFM (Google), and Moirai (Salesforce) across both short- and long-term forecasts.","tags":null,"title":"TiRex: Zero-Shot Forecasting Across Long and Short Horizons","type":"publication"},{"authors":["Maximilian Beck, Korbinian Pöppel, Markus Spanring, **Andreas Auer**, Oleksandra Prudnikova, Michael Kopp, Günter Klambauer, Johannes Brandstetter, Sepp Hochreiter "],"categories":null,"content":"","date":1733875200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1733875200,"objectID":"0b93749f7cee46a934f2a8450e8478db","permalink":"https://apointa.github.io/publication/2024-xlstm.html","publishdate":"2024-12-11T00:00:00Z","relpermalink":"/publication/2024-xlstm.html","section":"publication","summary":"In the 1990s, the constant error carousel and gating were introduced as the central ideas of the Long Short-Term Memory (LSTM). Since then, LSTMs have stood the test of time and contributed to numerous deep learning success stories, in particular they constituted the first Large Language Models (LLMs). However, the advent of the Transformer technology with parallelizable self-attention at its core marked the dawn of a new era, outpacing LSTMs at scale. We now raise a simple question: How far do we get in language modeling when scaling LSTMs to billions of parameters, leveraging the latest techniques from modern LLMs, but mitigating known limitations of LSTMs? Firstly, we introduce exponential gating with appropriate normalization and stabilization techniques. Secondly, we modify the LSTM memory structure, obtaining: (i) sLSTM with a scalar memory, a scalar update, and new memory mixing, (ii) mLSTM that is fully parallelizable with a matrix memory and a covariance update rule. Integrating these LSTM extensions into residual block backbones yields xLSTM blocks that are then residually stacked into xLSTM architectures. Exponential gating and modified memory structures boost xLSTM capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models, both in performance and scaling.","tags":null,"title":"xLSTM: Extended Long Short-Term Memory","type":"publication"},{"authors":["**Andreas Auer**, Martin Gauch, Frederik Kratzert, Grey Nearing, Sepp Hochreiter, Daniel Klotz "],"categories":null,"content":"","date":1726099200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1726099200,"objectID":"3416988e5a2770926d749c7b1155f48a","permalink":"https://apointa.github.io/publication/2024-data-centric.html","publishdate":"2024-09-12T00:00:00Z","relpermalink":"/publication/2024-data-centric.html","section":"publication","summary":"Uncertainty estimates are fundamental to assess the reliability of predictive models in hydrology. We use the framework of Conformal Prediction to investigate the impact of temporal and spatial information on uncertainty estimates within hydrological predictions. Integrating recent information significantly enhances overall uncertainty predictions, even with substantial gaps between updates. While local information yields good results on average, it proves insufficient for peak flow predictions. Incorporating global information improves the accuracy of peak flow bounds, corroborating findings from related studies. Overall, the study underscores the importance of continuous data updates and the integration of global information for robust and efficient uncertainty estimation.","tags":null,"title":"A data-centric perspective on the information needed for hydrological uncertainty predictions","type":"publication"},{"authors":["Maximilian Beck, Korbinian Pöppel, Markus Spanring, **Andreas Auer**, Oleksandra Prudnikova, Michael Kopp, Günter Klambauer, Johannes Brandstetter, Sepp Hochreiter "],"categories":null,"content":"","date":1720656e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720656e3,"objectID":"6ff2f8a863f20eb28b24cdfe1d622150","permalink":"https://apointa.github.io/publication/2024-xlstm-workshop.html","publishdate":"2024-07-11T00:00:00Z","relpermalink":"/publication/2024-xlstm-workshop.html","section":"publication","summary":"In the 1990s, the constant error carousel and gating were introduced as the central ideas of the Long Short-Term Memory (LSTM). Since then, LSTMs have stood the test of time and contributed to numerous deep learning success stories, in particular they constituted the first Large Language Models (LLMs). However, the advent of the Transformer technology with parallelizable self-attention at its core marked the dawn of a new era, outpacing LSTMs at scale. We now raise a simple question: How far do we get in language modeling when scaling LSTMs to billions of parameters, leveraging the latest techniques from modern LLMs, but mitigating known limitations of LSTMs? Firstly, we introduce exponential gating with appropriate normalization and stabilization techniques. Secondly, we modify the LSTM memory structure, obtaining: (i) sLSTM with a scalar memory, a scalar update, and new memory mixing, (ii) mLSTM that is fully parallelizable with a matrix memory and a covariance update rule. Integrating these LSTM extensions into residual block backbones yields xLSTM blocks that are then residually stacked into xLSTM architectures. Exponential gating and modified memory structures boost xLSTM capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models, both in performance and scaling.","tags":null,"title":"xLSTM: Extended Long Short-Term Memory","type":"publication"},{"authors":null,"categories":null,"content":"Test\n","date":1682294400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1682294400,"objectID":"a958a57ca9780c2aa77d75e59c0581c0","permalink":"https://apointa.github.io/news/page-created.html","publishdate":"2023-04-24T00:00:00Z","relpermalink":"/news/page-created.html","section":"news","summary":"I have a page now!","tags":null,"title":"I have a page now!","type":"news"},{"authors":["**Andreas Auer**, Martin Gauch, Daniel Klotz, Sepp Hochreiter "],"categories":null,"content":"","date":1679443200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1679443200,"objectID":"d89cbee4df478c4196dde0449e8bcc92","permalink":"https://apointa.github.io/publication/2023-hopcpt.html","publishdate":"2023-03-22T00:00:00Z","relpermalink":"/publication/2023-hopcpt.html","section":"publication","summary":"To quantify uncertainty, conformal prediction methods are gaining continuously more interest and have already been successfully applied to various domains. However, they are difficult to apply to time series as the autocorrelative structure of time series violates basic assumptions required by conformal prediction. We propose HopCPT, a novel conformal prediction approach for time series that not only copes with temporal structures but leverages them. We show that our approach is theoretically well justified for time series where temporal dependencies are present. In experiments, we demonstrate that our new approach outperforms state-of-the-art conformal prediction methods on multiple real-world time series datasets from four different domains.","tags":null,"title":"Conformal Prediction for Time Series with Modern Hopfield Networks","type":"publication"},{"authors":["**Andreas Auer**"],"categories":null,"content":"","date":1647561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1647561600,"objectID":"74e62915525e19aa733e2ee6f18131b4","permalink":"https://apointa.github.io/publication/2022-vtkgnn.html","publishdate":"2022-03-18T00:00:00Z","relpermalink":"/publication/2022-vtkgnn.html","section":"publication","summary":"Temporal Knowledge Graphs, a form of a graph-structured knowledge bases, gained increased interest in both academia and industry in the last years. Most Knowledge Graphs suffer from incompleteness, ie from facts which are valid in the real world that are not represented in the Knowledge Graph at hand. A set of methods to counter this problem are Knowledge Graph Embeddings, which learn vector embeddings for the knowledge graph elements in a way that the underlying structure of the Knowledge Graph is preserved and utilize the learned embeddings to predict missing facts. Knowledge Graph Embeddings show good results but allow only transductive link prediction. In addition their integration of prior information of the entities as well as of temporal information is insufficient and they suffer from scalability problems for large graphs. In this thesis we propose the temporal knowledge graph neural network VT-KGNN. The model overcomes the limitations of Knowledge Graph Embeddings by using a multirelational adaption of local message passing graph neural networks as encoder and a Knowledge Graph Embedding score function as decoder. VT-KGNN especially focuses on the integration of temporal facts, more concretely on facts annotated with a valid time. It does not rely on snapshots but utilizes the temporal information by directly encoding it as a feature used in the Graph Neural Network. The model considers the start and end time of a fact individually, as well as in combination in the form of its duration. In addition it can utilize temporal relationships between facts that share entities","tags":null,"title":"VT-KGNN: A Valid Time Knowledge Graph Neural Network","type":"publication"}]